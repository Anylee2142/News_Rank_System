	 카드뉴스  인간은 로봇에게 윤리를 가르칠 수 있을까 

  과학동아 기자들이 기록한 특별부록  스티븐 호킹 



인공지능의 발달로 스스로 판단해 화기를 발사하는 전투용 로봇이 현실화 되고 있습니다. 이는 인간의 통제를 벗어나 재앙을 가져올까요  윤리적 시스템을 도입해 미리 대비할 수 있는 걸까요 

스티븐 호킹 박사를 비롯한 학자  일론 머스크 등의 테크업계 리더들은 인공지능 AI  기술의 급성장에 따르는 잠재적 위험을 경고해 왔습니다.

 과학동아 기자들이 기록한 특별부록  스티븐 호킹 

가장 논란이 되는 것은  군사용 전투로봇.  전장에서 로봇이 활용된 지는 10년이 훌쩍 지났지만  이제 로봇 스스로 판단해 화기 사용을 결정 할 수 있을 만큼 고도화 됐습니다.

많은 군사전문가들은 전투로봇 마르스에 우려를 표했죠. 마르스의 경우 인간병사가 원격으로 제어하고 작동 범위도 800m로 제한 돼 있지만 인공지능 발전에 따라 치명적으로 변신할 가능성이 크다는 것입니다.

이런 로봇들은 이른바  킬러 로봇 이라 불리죠. 사람의 간섭없이 공격이 가능한 완전 자율무기를 뜻합니다.  윤리적 민감성 을 갖추지 못한 채 개발된 로봇이 인간에게 미치는 해를 예측하기 어렵습니다.

로봇은 아직 장난감 총과 진짜를 구분 못한다

킬러로봇에 대한 논란의 핵심은 로봇은 사람에게 총을 쏠지 판단할 만큼 충분히 똑똑하거나 윤리적이지 않다는 것입니다.

킬러로봇이 아이인지 어른인지  총기 소지 여부  군복 착용 여부 3가지 항목으로 민간인과 군인을 비교하도록 개발됐다면  소년이 장난감 총을 들고 녹색옷을 입었을 때 군인으로 오인하기 쉽습니다.

로봇에게 윤리를 가르치는 게 가능할까  걸음마 단계이긴 하지만 이러한 상황에서 윤리적 소프트웨어를 만들려는 시도도 있습니다. 책  왜 로봇의 도덕인가 에서 소개한 접근법을 소개합니다.

로봇윤리 세 가지 접근법

1. 논리기반 접근법  큰 원칙을 알려줌

인공지능에 넣는  행동규칙  대신  생명은 소중하다 는 큰 원칙을 넣는다. 실제  최대다수의 최대행복 이라는  공리주의 원칙  원칙에 따라 결정을 내리도록 프로그래밍된 돌봄 로봇은 환자의 생명유지장치를 끌지 여부를 이에 따라 결정할 것이다. 사람의 사고체계와 비슷하지만 사람도 결정을 내리기 어렵고 복잡한 상황에는 적합하지 않다

2. 사례기반 접근법  인간이 내린 판단을 배움

의학윤리 전문가 시스템에 따라서 여러 의무가 상충할 때 과거의 구체적인 사례에서 인간 의료전문가의 결정을 비교 평가한다. 환자를 돌볼 의무와 환자의 자율성을 존중할 딜레마 사이에서 로봇은 두 의무에 할당된 값을 체크한다. 할당된 값이 정해진 값을 넘어가게 되면 즉 환자가 위험한 상태가 되면  주치의에게 연락한다.

3. 다중행위자 접근법  다양한 경우의 수 계산

윤리적 판단을 할 수 있는 여러 행위자들의 상호작용을 시뮬레이션하므로 상황에 따라 다른 의도를 가지고 행동할 수 있다. 이런  다중행위자 플랫폼 에서는 환자와 의사  간호사와  보험회사 등 다양한 사람이 환자의 개인정보에 접속할 때 누구에게 어떤 권리를 주는 것이 바람직한지 다양한 경우의 수를 계산하고 시뮬레이션 할 수 있다. .

인류의 역사에서 기술윤리는 늘 해당 기술보다 훨씬 늦게 발달해 왔습니다.

부작용과 해악을 줄이고 기술의 긍정적 확산을 위해 고민이 필요한 때입니다.

참고   과학동아 2014년 10월호  로봇에게 윤리를 가르칠 수 있을까 

이미지 출처   GIB
